{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Step 1: Setup (Colab Environment)"
      ],
      "metadata": {
        "id": "RSORZ0DXTER6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Install Dependencies"
      ],
      "metadata": {
        "id": "XYh5sr6yXhWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers torch"
      ],
      "metadata": {
        "id": "u5oA_R6RTGCa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch"
      ],
      "metadata": {
        "id": "CHh7e35RZ8Fq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model name\n",
        "model_name = \"google/t5-v1_1-large\"\n",
        "\n",
        "# Load the tokenizer and model\n",
        "# This will download the model weights (approx. 45GB) if not already cached.\n",
        "# Be prepared for a significant download and memory usage.\n",
        "print(f\"Loading tokenizer and model for {model_name}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, legacy=False)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "irZAlQN8EGHB",
        "outputId": "76a80f6b-4a9c-4dc1-bdaa-45f4763a84c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer and model for google/t5-v1_1-large...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"Model loaded on {device}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ai9vYVB7VFSk",
        "outputId": "7f06e821-e412-4b4a-e098-57a14c190ca2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(task_prefix, input_text, max_new_tokens=50):\n",
        "    input_text = f\"{task_prefix}: {input_text}\"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=512,\n",
        "        padding=False,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    model.to(device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        num_beams=4,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    num_output_tokens = len(tokenizer.tokenize(output_text))\n",
        "    print(f\"üìù Output:\\n{output_text}\")\n",
        "    print(f\"‚è± Tokens generated: {num_output_tokens} / {max_new_tokens}\")\n",
        "\n",
        "    return output_text\n"
      ],
      "metadata": {
        "id": "EYczFFwbG74T"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Cases"
      ],
      "metadata": {
        "id": "oyd78hBR0geT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Summarization ---\")\n",
        "article = \"\"\"\n",
        "Transformer models, first introduced in the 2017 paper \"Attention Is All You Need,\" have revolutionized the field of Natural Language Processing (NLP). Unlike traditional recurrent neural networks (RNNs) or convolutional neural networks (CNNs), Transformers rely entirely on an attention mechanism to draw global dependencies between input and output. This architecture allows for parallelization of computation, significantly speeding up training times compared to sequential models.\n",
        "\n",
        "Key components of a Transformer include multi-head self-attention mechanisms and position-wise feed-forward networks. Self-attention enables the model to weigh the importance of different words in the input sequence when processing a specific word, capturing contextual relationships effectively. Positional encodings are added to the input embeddings to provide information about the relative or absolute position of tokens in the sequence, as the self-attention mechanism itself is permutation-invariant.\n",
        "\n",
        "The original Transformer proposed an encoder-decoder structure. The encoder maps an input sequence of symbol representations to a sequence of continuous representations. The decoder then generates an output sequence one symbol at a time, taking the encoder's output and previously generated symbols as input. This architecture proved highly effective for tasks like machine translation.\n",
        "\n",
        "Later innovations built upon this foundation. BERT (Bidirectional Encoder Representations from Transformers), for instance, adopted an encoder-only architecture and focused on bidirectional context for understanding tasks like masked language modeling. GPT (Generative Pre-trained Transformer) models, on the other hand, are decoder-only and excel at generative tasks by predicting the next token in a sequence. The T5 family of models further unified various NLP tasks into a \"text-to-text\" framework, where every problem is converted into a text input and text output format.\n",
        "\"\"\"\n",
        "summary_prefix = \"summarize the following article\"\n",
        "generated_summary = generate_text(summary_prefix, article.strip(), max_new_tokens=150)\n",
        "print(f\"Original Article:\\n{article[:50]}...\\n\")\n",
        "print(f\"Generated Summary:\\n{generated_summary}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHy6fn-e0bA1",
        "outputId": "d206060d-e5d5-46b8-9387-4ac7a27257eb"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Summarization ---\n",
            "üìù Output:\n",
            ". Abstract: Transformer models have revolutionized the field of Natural Language Processing (NLP). Here's a brief introduction to Transformers.... Text to text. T5 family of models. To briefly and briefly.. These models focus on predicting the next token in an input sequence.. The resulting models are optimized for text-to-text tasks... A text input and text output format. Finally, the T7 family. The T6 family, on the other hand\n",
            "‚è± Tokens generated: 99 / 150\n",
            "Original Article:\n",
            "\n",
            "Transformer models, first introduced in the 2017 ...\n",
            "\n",
            "Generated Summary:\n",
            ". Abstract: Transformer models have revolutionized the field of Natural Language Processing (NLP). Here's a brief introduction to Transformers.... Text to text. T5 family of models. To briefly and briefly.. These models focus on predicting the next token in an input sequence.. The resulting models are optimized for text-to-text tasks... A text input and text output format. Finally, the T7 family. The T6 family, on the other hand\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Question Answering ---\")\n",
        "context = \"The Eiffel Tower is located in Paris, France. It was designed by Gustave Eiffel.\"\n",
        "question = \"Where is the Eiffel Tower located?\"\n",
        "qa_prefix = \"question\"\n",
        "qa_input = f\"question: {question} context: {context}\"\n",
        "generated_answer = generate_text(qa_prefix, qa_input, max_new_tokens=20)\n",
        "\n",
        "print(f\"Context: {context}\")\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Generated Answer: {generated_answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fn-k0ChV0lIT",
        "outputId": "16331f42-eb05-4ec5-adce-ec1029b73637"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Question Answering ---\n",
            "üìù Output:\n",
            ": The Eiffel Tower is located in Paris, France.???\n",
            "‚è± Tokens generated: 14 / 20\n",
            "Context: The Eiffel Tower is located in Paris, France. It was designed by Gustave Eiffel.\n",
            "Question: Where is the Eiffel Tower located?\n",
            "Generated Answer: : The Eiffel Tower is located in Paris, France.???\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Translation (English to French) ---\")\n",
        "english_sentence = \"This is a beautiful day.\"\n",
        "translation_prefix = \"translate English to French\"\n",
        "generated_translation = generate_text(translation_prefix, english_sentence, max_new_tokens=50)\n",
        "\n",
        "print(f\"English: {english_sentence}\")\n",
        "print(f\"French: {generated_translation}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EobVzSGO0ds7",
        "outputId": "9b3d2d23-b603-4c37-da53-84013a853c2c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Translation (English to French) ---\n",
            "üìù Output:\n",
            "A beautiful day.\n",
            "‚è± Tokens generated: 4 / 50\n",
            "English: This is a beautiful day.\n",
            "French: A beautiful day.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Text Classification (Sentiment Analysis) ---\")\n",
        "# Note: For classification, you'd typically fine-tune. Here, we're trying\n",
        "# to prompt it zero-shot, which might not be as reliable as Flan-T5.\n",
        "sentiment_text = \"I absolutely loved this movie, it was fantastic!\"\n",
        "sentiment_prefix = \"classify the sentiment of this text as positive, negative, or neutral\"\n",
        "generated_sentiment = generate_text(sentiment_prefix, sentiment_text, max_new_tokens=10)\n",
        "\n",
        "print(f\"Text: {sentiment_text}\")\n",
        "print(f\"Sentiment: {generated_sentiment}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFPEqbpf0o6g",
        "outputId": "7d05513a-0e8e-4e3e-fff5-08c0d0457c2b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Text Classification (Sentiment Analysis) ---\n",
            "üìù Output:\n",
            "the sentiment of this text as positive.\n",
            "‚è± Tokens generated: 8 / 10\n",
            "Text: I absolutely loved this movie, it was fantastic!\n",
            "Sentiment: the sentiment of this text as positive.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Paraphrasing ---\")\n",
        "original_sentence = \"The cat sat on the mat.\"\n",
        "paraphrase_prefix = \"paraphrase\"\n",
        "generated_paraphrase = generate_text(paraphrase_prefix, original_sentence, max_new_tokens=20)\n",
        "print(f\"Original: {original_sentence}\")\n",
        "print(f\"Paraphrase: {generated_paraphrase}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjkDKUx30qMC",
        "outputId": "82b2ca84-7966-4610-c98b-af4249dcf45a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Paraphrasing ---\n",
            "üìù Output:\n",
            "the mat.\n",
            "‚è± Tokens generated: 3 / 20\n",
            "Original: The cat sat on the mat.\n",
            "Paraphrase: the mat.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8_YYdRXv67xw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}